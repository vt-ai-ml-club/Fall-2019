{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vt-ai-ml/fall2019-meetings/blob/master/SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo1x_gPzsM_o",
        "colab_type": "text"
      },
      "source": [
        "# Support Vector Machines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q__SSJ_tUVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import statements, must always run\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import feature_extraction, model_selection\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAisDkmk7hq1",
        "colab_type": "text"
      },
      "source": [
        "## General Idea"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-hI2fGatM-k",
        "colab_type": "text"
      },
      "source": [
        "Motivating example:\n",
        "Assume we have some fruits pears (green) and apples (blue). Let's say we plot them by their weight (x-axis). We then find an unknown fruit (black). Is it a pear or an apple?\n",
        "\n",
        "(Note: this is a binary classification problem; that is, we want to classify a data point as one of two labels)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru8e2pMVtTdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gx = [1, 2, 3]\n",
        "bx = [5, 6, 7]\n",
        "y = [1, 1, 1]\n",
        "\n",
        "plt.scatter(gx, y, c='green')\n",
        "plt.scatter(bx, y, c='blue')\n",
        "plt.scatter([4.5], [1], c='black')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhGHIw0GwiM-",
        "colab_type": "text"
      },
      "source": [
        "Solution: We can draw a line on the graph. Everyone on the left side we can call a pear, everything on the right side we call an apple."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e9tNJp_wA4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gx = [1, 2, 3]\n",
        "bx = [5, 6, 7]\n",
        "y = [1, 1, 1]\n",
        "\n",
        "plt.scatter(gx, y, c='green')\n",
        "plt.scatter(bx, y, c='blue')\n",
        "plt.plot([3.5, 3.5], [.995, 1.005], 'orange')\n",
        "plt.scatter([4.5], [1], c='black')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJm54ZfawxYT",
        "colab_type": "text"
      },
      "source": [
        "Are there other possible lines we could draw instead?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUo8bPkTujxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gx = [1, 2, 3]\n",
        "bx = [5, 6, 7]\n",
        "y = [1, 1, 1]\n",
        "\n",
        "plt.scatter(gx, y, c='green')\n",
        "plt.scatter(bx, y, c='blue')\n",
        "\n",
        "plt.plot([3.5, 3.5], [.995, 1.005], 'orange')\n",
        "plt.plot([4, 4], [.995, 1.005], 'red')\n",
        "plt.plot([4.5, 4.5], [.995, 1.005], 'yellow')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCjapf4Ew46o",
        "colab_type": "text"
      },
      "source": [
        "How do we determine which line is the best one? Why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOCMLDX8vHaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gx = [1, 2, 3]\n",
        "bx = [5, 6, 7]\n",
        "y = [1, 1, 1]\n",
        "\n",
        "plt.scatter(gx, y, c='green')\n",
        "plt.scatter(bx, y, c='blue')\n",
        "\n",
        "plt.plot([4, 4], [.995, 1.005], 'red')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0YBQOZ5y0xh",
        "colab_type": "text"
      },
      "source": [
        "Solution: Want to maximize the \"margin\".\n",
        "\n",
        "Margin: distance between the line (hyperplane) and the closest data points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyp1oBNqyx7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gx = [1, 2, 3]\n",
        "bx = [5, 6, 7]\n",
        "y = [1, 1, 1]\n",
        "\n",
        "plt.scatter(gx, y, c='green')\n",
        "plt.scatter(bx, y, c='blue')\n",
        "\n",
        "plt.plot([3, 3], [.995, 1.005], 'orange', ls='--')\n",
        "plt.plot([4, 4], [.995, 1.005], 'red')\n",
        "plt.plot([5, 5], [.995, 1.005], 'yellow', ls='--')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5PYv2Ggz7AN",
        "colab_type": "text"
      },
      "source": [
        "Why is maximizing the margin a good approach?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjQ3mh1G0bo4",
        "colab_type": "text"
      },
      "source": [
        "Intuition: We want to pick a line to divide the data into two parts, but also far away from the data points. Then, if we get a data point closer  to the hyperplane (the \"middle\"), the point will still be on the right side."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6EO61v61PXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gx = [1, 2, 3]\n",
        "bx = [5, 6, 7]\n",
        "y = [1, 1, 1]\n",
        "\n",
        "plt.scatter(gx, y, c='green')\n",
        "plt.scatter(bx, y, c='blue')\n",
        "\n",
        "plt.plot([3.5, 3.5], [.995, 1.005], 'orange')\n",
        "plt.plot([4, 4], [.995, 1.005], 'red')\n",
        "plt.plot([4.5, 4.5], [.995, 1.005], 'yellow')\n",
        "plt.scatter([3.75], [1], c='black')\n",
        "plt.scatter([4.25], [1], c='black')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGpQLYJm1fnR",
        "colab_type": "text"
      },
      "source": [
        "So what if we have more than just weight? That is, what if we want to classify based on more than one feature?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fqAX2s33pwU",
        "colab_type": "text"
      },
      "source": [
        "Solution: Add more dimensions! We make each dimension represent a new feature and we (still) use a hyperplane to split the data. [Multiple dimension examples.](https://blog.statsbot.co/support-vector-machines-tutorial-c1618e635e93)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evYq5yMA5X7g",
        "colab_type": "text"
      },
      "source": [
        "So, if have want to classify by $n$ features, we plot the data in $n$ dimensions where each part of the coordinate is a feature. Then we draw the hyperplane which will be the equivalent of a \"line\" to separate the data in $n$ dimensions. (a line in 2D, plane in 3D, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxkkTxge7xuM",
        "colab_type": "text"
      },
      "source": [
        "## Implementation Details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4usmhhf70E8",
        "colab_type": "text"
      },
      "source": [
        "Kernel:\n",
        "\n",
        "We we've been drawing a straight line to separate the data, but there are more options:\n",
        "* linear (line)\n",
        "* polynomial (e.g. $x^2,x^3$)\n",
        "* radial basis function (RBF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-rraBgyCqup",
        "colab_type": "text"
      },
      "source": [
        "Polynomial Kernel Example:\n",
        "![Polynomial Example](https://i.imgur.com/rlCdLqV.jpg)\n",
        "\n",
        "RBF Kernel Example:\n",
        "![RBF Example](https://i.imgur.com/edNXrb9.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sbmcdbuq7_6g",
        "colab_type": "text"
      },
      "source": [
        "## Advantages/Disadvantages and Use Cases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpjMcDpdEt3B",
        "colab_type": "text"
      },
      "source": [
        "Advantages:\n",
        "* Good for both linearly and non-linearly separable data\n",
        "* Good in both cases where the labels are known and unknown (you can still find trends even if the label is unknown)\n",
        "* Helps avoid overfitting\n",
        "* Efficient methods of creating\n",
        "\n",
        "Disadvantage:\n",
        "* Choosing the kernel to use\n",
        "* Choosing other parameters (C and gamma, see Extra Info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jjl1eaqrD-hV",
        "colab_type": "text"
      },
      "source": [
        "Example use cases of SVM:\n",
        "* Face detection\n",
        "* Text categorization\n",
        "* Bioinformations (used for  protein classification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1gntJBi8G8b",
        "colab_type": "text"
      },
      "source": [
        "## Spam Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PGUcnuOHru9",
        "colab_type": "text"
      },
      "source": [
        "What's commonly used as the opposite of spam (for email)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x8KM7tFG1lB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load/display data\n",
        "data_url = 'https://raw.githubusercontent.com/ParakweetLabs/EmailIntentDataSet/master/src/resources/Ask0729-fixed.txt'\n",
        "df = pd.read_csv(data_url, sep='\\t', names=['Ham?', 'Subject Line'])\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz_HwgH9HxfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How many of each type do we have?\n",
        "print(pd.value_counts(df['Ham?']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9K-rslqINw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print the most common words\n",
        "most_common_words = Counter(' '.join(df['Subject Line']).split()).most_common(10)\n",
        "words_df = pd.DataFrame.from_dict(most_common_words)\n",
        "words_df.plot.bar(x=0, y=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMYYWrxTJ3AH",
        "colab_type": "text"
      },
      "source": [
        "Basic NLP:\n",
        "\n",
        "Stop words: common words which are normally filtered out, because they have no real significance in our data (e.g., \"the\", \"to\", \"and\")\n",
        "\n",
        "SVM only works with numbers for coordinates. Solution: convert words to numbers!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I92RFGCZJ2Lw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove stop words and map words to numbers\n",
        "f = feature_extraction.text.CountVectorizer(stop_words='english')\n",
        "X = f.fit_transform(df['Subject Line'])\n",
        "Y = df['Ham?'].map({'No': 0, 'Yes': 1})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EypBffwKzIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create all the kernels\n",
        "rbf = SVC(kernel='rbf', gamma='auto')\n",
        "linear = SVC(kernel='linear', gamma='auto')\n",
        "poly = SVC(kernel='poly', gamma='auto')\n",
        "\n",
        "# train data\n",
        "rbf.fit(X, Y)\n",
        "linear.fit(X, Y)\n",
        "poly.fit(X, Y)\n",
        "\n",
        "# see accuracy from training\n",
        "print('rbf', rbf.score(X, Y))\n",
        "print('linear', linear.score(X, Y))\n",
        "print('poly', poly.score(X, Y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpHgf4788YLP",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "* [Do SVMs only do binary classification?\n",
        "](https://www.quora.com/Do-SVMs-only-do-binary-classification)\n",
        "* [What is the intuition behind margin in SVM?\n",
        "](https://www.quora.com/What-is-the-intuition-behind-margin-in-SVM)\n",
        "* [Support Vector Machines Tutorial](https://data-flair.training/blogs/svm-support-vector-machine-tutorial/)\n",
        "* [Support Vector Machines for Classification\n",
        "](https://mubaris.com/posts/svm/)\n",
        "* [Real-Life Applications of SVM](https://data-flair.training/blogs/applications-of-svm/)\n",
        "\n"
      ]
    }
  ]
}