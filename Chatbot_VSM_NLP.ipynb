{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Chatbot_VSM_NLP.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vt-ai-ml/fall2019-meetings/blob/master/Chatbot_VSM_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-UVUK-eHQiy",
        "colab_type": "text"
      },
      "source": [
        "## Building a chatbot using a VSM\n",
        "\n",
        "Reference: https://medium.com/analytics-vidhya/building-a-simple-chatbot-in-python-using-nltk-7c8c8215ac6e"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8649OI-tHRt5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download data\n",
        "import requests\n",
        "url = 'https://raw.githubusercontent.com/vt-ai-ml/fall2019-meetings/master/data/chatbot_corpus.txt'\n",
        "data = requests.get(url).text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8Va76yIHQi0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "splits = data.split('\\n')\n",
        "splits = splits[:-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUHDkN_fHQi4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary = {}\n",
        "questions_list = []\n",
        "\n",
        "# Creating a key, value pair as <question,response>\n",
        "for i in range(0, len(splits), 2):\n",
        "    question = splits[i].replace('-','').strip()\n",
        "    response = splits[i+1].replace('-','').strip()\n",
        "    \n",
        "    if question in dictionary:\n",
        "        dictionary[question].append(response)\n",
        "    else:\n",
        "        dictionary[question] = [response]\n",
        "        questions_list.append(question)\n",
        "        \n",
        "print(questions_list[-5:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Hqy8p-UHQi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "def normal_lemma_tokens(text):\n",
        "    tokens = nltk.word_tokenize(text.lower().translate(remove_punct_dict))\n",
        "    return [lemmer.lemmatize(token) for token in tokens]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lk81VqBCHQi8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "def select_random_response(question):\n",
        "    return random.choice(dictionary[question])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T23Ynw5NHQi-",
        "colab_type": "text"
      },
      "source": [
        "### Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        ">**Term Frequency**: scores the frequency of a word in the current document.\n",
        "\n",
        "$$ TF(w,d) = \\frac{\\text{# of times w appears in d}}{\\text{total # of words in the d}} $$\n",
        "\n",
        ">**Inverse Document Frequency**: scores how rare the word is across all documents.\n",
        ">+ A word with a *low* IDF score is a *common* word.\n",
        ">+ A word with a *high* IDF score is a *rare* word.\n",
        "\n",
        "$$ IDF(w,D) = \\log{(\\frac{\\text{total # of documents}}{\\text{# of documents containing w}})} $$\n",
        "\n",
        "\n",
        "\n",
        "### Word Similarity \n",
        ">**Cosine similarity**: is the measure of similarity between two vectors. In our case, the similarity between two questions.\n",
        "\n",
        "$$ cos(u,v) = \\frac{u \\cdot v}{\\left\\lVert u \\right\\lVert \\left\\lVert v \\right\\lVert} $$\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/vt-ai-ml/fall2019-meetings/raw/master/data/chatbot_cosine_image.png\" align=\"left\" style=\"width:250px;height:250px;\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP3JBnK0HQi_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def respond(user_question):\n",
        "    questions_list.append(user_question)\n",
        "    \n",
        "    # Maps user response and questions into TF-IDF vector space, where each word is a dimension\n",
        "    tf_idf_matrix = tf_idf_vector_space.fit_transform(questions_list)\n",
        "    \n",
        "    # Calculate the cosine similarity of user's response with other vectors (question) in the vector space\n",
        "    user_question_tf_idf = tf_idf_matrix[-1]\n",
        "    similarity_score = cosine_similarity(user_question_tf_idf, tf_idf_matrix[:-1])\n",
        "    \n",
        "    # Find the most similar vector (question) to our user's response\n",
        "    highest_tf_idf_idx = similarity_score.argmax()\n",
        "    highest_tf_idf = similarity_score.max()\n",
        "    \n",
        "    # Find appropriate response\n",
        "    if(highest_tf_idf == 0): # no similarity\n",
        "        robo_response = \"I don't understand what you're saying.\"\n",
        "    else:\n",
        "        robo_response = select_random_response(questions_list[highest_tf_idf_idx])\n",
        "        \n",
        "    questions_list.remove(user_question)\n",
        "    return robo_response"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxDfrBIxHQjB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_idf_vector_space = TfidfVectorizer(tokenizer=normal_lemma_tokens, stop_words='english')\n",
        "\n",
        "print(\"Chatbot is on! Type 'exit' to turn off the chatbot.\")\n",
        "while(True):\n",
        "    user_question = input().lower().translate(remove_punct_dict)\n",
        "    \n",
        "    if(user_question == 'exit'):\n",
        "        print('Chatbot is now off!')\n",
        "        break       \n",
        "    else:\n",
        "        print(\"BOT: \", respond(user_question), '\\n')  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3cgoFFaHQjD",
        "colab_type": "text"
      },
      "source": [
        "### Questions to ask your chatbot\n",
        "* How do you do?\n",
        "* Tell me a joke\n",
        "* What is a chat robot?\n",
        "* What can you eat?"
      ]
    }
  ]
}